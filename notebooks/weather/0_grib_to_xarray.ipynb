{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "import shapely.plotting\n",
    "import geojson\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2022-02-01\"\n",
    "end_date = \"2024-04-08\"\n",
    "list_date = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "list_date = list_date.strftime(\"%Y-%m-%d\")\n",
    "cases_forecast = {\n",
    "    \"J+0\": [\"00H12H\", \"13H24H\"],\n",
    "    \"J+1\": [\"25H36H\", \"37H48H\"],\n",
    "    \"J+2\": [\"49H60H\", \"61H72H\"],\n",
    "    \"J+3\": [\"73H84H\", \"85H96H\"],\n",
    "}\n",
    "cases = [\n",
    "    \"00H12H\",\n",
    "    \"13H24H\",\n",
    "]  # \"25H36H\", \"37H48H\", \"49H60H\", \"61H72H\", \"73H84H\", \"85H96H\", \"97H102H\"]\n",
    "filename_to_save_template = (\n",
    "    \"/shared/home/antoine-2etavant/data/arpege/{date}_SP1_{case}.grib2\"\n",
    ")\n",
    "\n",
    "\n",
    "filename = filename_to_save_template.format(date=list_date[-1], case=cases[1])\n",
    "\n",
    "number_of_files_to_process = 3\n",
    "filenames_morning = [\n",
    "    filename_to_save_template.format(date=date, case=cases[0])\n",
    "    for date in list_date[:number_of_files_to_process]\n",
    "]\n",
    "filenames_afternoon = [\n",
    "    filename_to_save_template.format(date=date, case=cases[1])\n",
    "    for date in list_date[:number_of_files_to_process]\n",
    "]\n",
    "\n",
    "filenames_morning_exist = [\n",
    "    filename for filename in filenames_morning if Path(filename).exists()\n",
    "]\n",
    "filenames_afternoon_exist = [\n",
    "    filename for filename in filenames_afternoon if Path(filename).exists()\n",
    "]\n",
    "\n",
    "filenames_morning = filenames_morning_exist\n",
    "filenames_afternoon = filenames_afternoon_exist\n",
    "\n",
    "try:\n",
    "    ## remove a file that is not working properly\n",
    "    filenames_afternoon.remove(\n",
    "        \"/shared/home/antoine-2etavant/data/arpege/2023-08-20_SP1_13H24H.grib2\"\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "save_parquet = number_of_files_to_process == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a list a Grib files\n",
    "\n",
    "The following section opens a list of grib files and merge them intoo one XArray.\n",
    "\n",
    "It requieres to av install Dask and CFGrid in addition to xarray, that can be install with\n",
    "```bash\n",
    "mamba install -y -c conda-forge dask xarray cfgrib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS_FILTER_SSPD = {\n",
    "    \"typeOfLevel\": \"surface\",\n",
    "    \"cfVarName\": \"ssrd\",\n",
    "}\n",
    "KEYS_FILTER_WIND = {\n",
    "    \"typeOfLevel\": \"heightAboveGround\",\n",
    "    \"level\": 10,\n",
    "    \"cfVarName\": \"si10\",\n",
    "}\n",
    "KEYS_FILTER_T2M = {\n",
    "    \"typeOfLevel\": \"heightAboveGround\",\n",
    "    \"level\": 2,\n",
    "    \"cfVarName\": \"t2m\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask if point inside metropole\n",
    "metropole_geojson_file = \"./notebooks/datascience/metropole.geojson\"\n",
    "metropole = geojson.load(open(metropole_geojson_file))\n",
    "polys_france = [Polygon(geo[0]) for geo in metropole[\"geometry\"][\"coordinates\"]]\n",
    "areas = [p.area for p in polys_france]\n",
    "\n",
    "min_area = 0.2  # threshold to remove small islands\n",
    "polys_france = [p for p, area in zip(polys_france, areas) if area > min_area]\n",
    "print(f\"number of polygons: {len(polys_france)}\")\n",
    "poly_france = MultiPolygon(polys_france)\n",
    "# poly_france = polys_france[0]  # france metropolitan\n",
    "\n",
    "bouns = poly_france.envelope.bounds\n",
    "min_lon = bouns[0]\n",
    "max_lon = bouns[2]\n",
    "min_lat = bouns[1]\n",
    "max_lat = bouns[3]\n",
    "\n",
    "for poly in poly_france.geoms:\n",
    "    shapely.plotting.plot_polygon(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solar Flux\n",
    "\n",
    "This section reads the solar flux from the GRIB2 files, an uses XArray to process the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_no_step(ds):\n",
    "    # Check if 'step' is in the dataset's coordinates\n",
    "    # Used to find files that are not working properly\n",
    "    if \"step\" not in ds.coords:\n",
    "        display(ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "da_ssrd = xr.open_mfdataset(\n",
    "    filenames_afternoon,\n",
    "    engine=\"cfgrib\",\n",
    "    parallel=True,\n",
    "    backend_kwargs={\"filter_by_keys\": KEYS_FILTER_SSPD},\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\",\n",
    "    preprocess=drop_no_step,\n",
    ").ssrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssrd_france_large = da_ssrd.sel(step=np.timedelta64(1, \"D\")).where(\n",
    "    (da_ssrd.longitude > min_lon)\n",
    "    & (da_ssrd.longitude < max_lon)\n",
    "    & (da_ssrd.latitude > min_lat)\n",
    "    & (da_ssrd.latitude < max_lat),\n",
    "    drop=True,\n",
    ")\n",
    "ssrd_france_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sun_flux_large = ssrd_france_large.sum(dim=[\"latitude\", \"longitude\"])\n",
    "total_sun_flux_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sun_flux_large = total_sun_flux_large.to_dataframe()[[\"ssrd\"]]\n",
    "df_total_sun_flux_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_parquet:\n",
    "    df_total_sun_flux_large.to_parquet(\"sun_flux.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_sun_flux_large.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the mean wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_wind_10m_morning = xr.open_mfdataset(\n",
    "    filenames_morning,\n",
    "    engine=\"cfgrib\",\n",
    "    parallel=True,\n",
    "    backend_kwargs={\"filter_by_keys\": KEYS_FILTER_WIND},\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\",\n",
    ").si10\n",
    "\n",
    "da_wind_10m_afternoon = xr.open_mfdataset(\n",
    "    filenames_afternoon,\n",
    "    engine=\"cfgrib\",\n",
    "    parallel=True,\n",
    "    backend_kwargs={\"filter_by_keys\": KEYS_FILTER_WIND},\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\",\n",
    ").si10\n",
    "da_wind_10m = xr.concat(\n",
    "    [\n",
    "        da_wind_10m_morning,\n",
    "        da_wind_10m_afternoon.where(\n",
    "            da_wind_10m_afternoon.step < np.timedelta64(1, \"D\"), drop=True\n",
    "        ),\n",
    "    ],\n",
    "    dim=\"step\",\n",
    ")\n",
    "da_wind_10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_10m_large = da_wind_10m.where(\n",
    "    (da_wind_10m.longitude > min_lon)\n",
    "    & (da_wind_10m.longitude < max_lon)\n",
    "    & (da_wind_10m.latitude > min_lat)\n",
    "    & (da_wind_10m.latitude < max_lat),\n",
    "    drop=True,\n",
    ")\n",
    "wind_10m_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wind_10m_large = (\n",
    "    wind_10m_large.sum(dim=[\"latitude\", \"longitude\"])\n",
    "    .stack(time_step=(\"time\", \"step\"))\n",
    "    .set_index(time_step=\"valid_time\")\n",
    ")\n",
    "total_wind_10m_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_wind_10m_large = total_wind_10m_large.to_dataframe()[[\"si10\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_parquet:\n",
    "    df_total_wind_10m_large.to_parquet(\"total_wind_10m.parquet\")\n",
    "df_total_wind_10m_large.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating values in the Territory\n",
    "\n",
    "The Above analysis only looked at the large average / total variable.\n",
    "\n",
    "The idea of the two next sections is to select only the data inside a given territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that checks if a point is inside the Polygon\n",
    "def is_inside(lon, lat, polygons: MultiPolygon):\n",
    "    return polygons.contains(Point(lon, lat))\n",
    "\n",
    "\n",
    "def compute_mask_da(da: xr.DataArray, polygons: MultiPolygon):\n",
    "    mask = xr.apply_ufunc(\n",
    "        is_inside,\n",
    "        da.longitude,\n",
    "        da.latitude,\n",
    "        kwargs={\"polygons\": polygons},\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "    )\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_france_wind = compute_mask_da(wind_10m_large, poly_france)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(\n",
    "    mask_france_wind.values.T,\n",
    "    extent=[\n",
    "        mask_france_wind.longitude.min(),\n",
    "        mask_france_wind.longitude.max(),\n",
    "        mask_france_wind.latitude.min(),\n",
    "        mask_france_wind.latitude.max(),\n",
    "    ],\n",
    "    origin=\"upper\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Mask of France used for Excat mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mask to select the points from the dataset\n",
    "wind_10m_exact = wind_10m_large.where(mask_france_wind, drop=True)\n",
    "wind_10m_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first day\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(\n",
    "    1, 3, figsize=(15, 5), subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    ")\n",
    "extent = [min_lon, max_lon, min_lat, max_lat]\n",
    "ax1.imshow(\n",
    "    wind_10m_exact.isel(time=0, step=0),\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    extent=extent,\n",
    "    origin=\"upper\",\n",
    ")\n",
    "\n",
    "ax2.imshow(\n",
    "    wind_10m_large.isel(time=0, step=0),\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    extent=extent,\n",
    "    origin=\"upper\",\n",
    ")\n",
    "ax3.imshow(\n",
    "    da_wind_10m.isel(time=0, step=0),\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    extent=[\n",
    "        da_wind_10m.longitude.min(),\n",
    "        da_wind_10m.longitude.max(),\n",
    "        da_wind_10m.latitude.min(),\n",
    "        da_wind_10m.latitude.max(),\n",
    "    ],\n",
    "    origin=\"upper\",\n",
    ")\n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    ax.add_feature(cfeature.COASTLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wind_10m_exact = (\n",
    "    wind_10m_exact.sum(dim=[\"latitude\", \"longitude\"])\n",
    "    .stack(time_step=(\"time\", \"step\"))\n",
    "    .set_index(time_step=\"valid_time\")\n",
    ")\n",
    "total_wind_10m_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_wind_10m_exact = total_wind_10m_exact.to_dataframe()[[\"si10\"]]\n",
    "df_total_wind_10m_exact.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_parquet:\n",
    "    df_total_wind_10m_exact.to_parquet(\"total_wind_10m_exact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssrd_france_exact = ssrd_france_large.where(mask_france_wind, drop=True)\n",
    "ssrd_france_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssrd_daily_exact = ssrd_france_exact.sum(dim=[\"latitude\", \"longitude\"])\n",
    "ssrd_daily_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssrd_daily_exact = ssrd_daily_exact.to_dataframe()[[\"ssrd\"]]\n",
    "df_ssrd_daily_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_parquet:\n",
    "    df_ssrd_daily_exact.to_parquet(\"total_ssrd_daily_exact.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssrd_daily_exact.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each regions\n",
    "\n",
    "Now, we process each Regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./notebooks/datascience/regions.geojson\"\n",
    "polys = geojson.load(open(filename))\n",
    "for feature in polys[\"features\"]:\n",
    "    print(feature[\"properties\"][\"nom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_keep = [\n",
    "    \"Bretagne\",\n",
    "    \"Centre-Val de Loire\",\n",
    "    \"Grand Est\",\n",
    "    \"Hauts-de-France\",\n",
    "    \"Île-de-France\",\n",
    "    \"Normandie\",\n",
    "    \"Nouvelle-Aquitaine\",\n",
    "    \"Occitanie\",\n",
    "    \"Pays de la Loire\",\n",
    "    \"Provence-Alpes-Côte d'Azur\",\n",
    "    \"Bourgogne-Franche-Comté\",\n",
    "    \"Auvergne-Rhône-Alpes\",\n",
    "    \"Corse\",\n",
    "]\n",
    "\n",
    "polys_region = {}\n",
    "for feature in tqdm(polys[\"features\"]):\n",
    "    name = feature[\"properties\"][\"nom\"]\n",
    "    if name not in names_to_keep:\n",
    "        continue\n",
    "    if feature[\"geometry\"][\"type\"] == \"Polygon\":\n",
    "        polys_region[name] = Polygon(feature[\"geometry\"][\"coordinates\"][0])\n",
    "    elif feature[\"geometry\"][\"type\"] == \"MultiPolygon\":\n",
    "        # keeping the largest polygon\n",
    "        tmp_list = [Polygon(geo[0]) for geo in feature[\"geometry\"][\"coordinates\"]]\n",
    "        largest = max(tmp_list, key=lambda x: x.area)\n",
    "        polys_region[name] = Polygon(largest)\n",
    "\n",
    "\n",
    "all_polys = [poly for poly in polys_region.values()]\n",
    "# unpack the lists\n",
    "maxi_multi_poly = MultiPolygon(all_polys)\n",
    "for poly in maxi_multi_poly.geoms:\n",
    "    shapely.plotting.plot_polygon(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "for name, poly in polys_region.items():\n",
    "    mask = xr.apply_ufunc(\n",
    "        is_inside,\n",
    "        wind_10m_large.longitude,\n",
    "        wind_10m_large.latitude,\n",
    "        kwargs={\"polygons\": poly},\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "    )\n",
    "    masks[name] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all masks\n",
    "fig, ax = plt.subplots()\n",
    "colors = plt.cm.tab20.colors\n",
    "extent = [\n",
    "    mask.longitude.min(),\n",
    "    mask.longitude.max(),\n",
    "    mask.latitude.min(),\n",
    "    mask.latitude.max(),\n",
    "]\n",
    "for c, mask in zip(colors, masks.values()):\n",
    "    # plot the mask with the color c\n",
    "    values = mask.values.T.astype(float)\n",
    "    X = np.array([[c[0], c[1], c[2], alpha] for alpha in values.flatten()]).reshape(\n",
    "        values.shape + (4,)\n",
    "    )\n",
    "    ax.imshow(X, origin=\"upper\", cmap=\"viridis\", extent=extent)\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_title(\"Masks of the regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump({\"mask\": mask_france_wind, \"masks_regions\": masks}, open(\"mask.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ssrd = {}\n",
    "for name, mask in tqdm(masks.items()):\n",
    "    ssrd = ssrd_france_large.where(mask, drop=True)\n",
    "    ssrd_daily = ssrd.sum(dim=[\"latitude\", \"longitude\"]).to_dataframe()[\"ssrd\"]\n",
    "    results_ssrd[name] = ssrd_daily\n",
    "\n",
    "all_ssrd = pd.concat(results_ssrd, axis=1)\n",
    "if save_parquet:\n",
    "    all_ssrd.to_parquet(\"./notebooks/weather/all_ssrd_regions.parquet\")\n",
    "all_ssrd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_si10 = {}\n",
    "for name, mask in tqdm(masks.items()):\n",
    "    wind = wind_10m_large.where(mask, drop=True)\n",
    "    wind_daily = (\n",
    "        wind.sum(dim=[\"latitude\", \"longitude\"])\n",
    "        .stack(time_step=(\"time\", \"step\"))\n",
    "        .set_index(time_step=\"valid_time\")\n",
    "        .to_dataframe()[\"si10\"]\n",
    "    )\n",
    "    results_si10[name] = wind_daily\n",
    "\n",
    "all_si10 = pd.concat(results_si10, axis=1)\n",
    "if save_parquet:\n",
    "    all_si10.to_parquet(\"./notebooks/weather/all_si10_regions.parquet\")\n",
    "all_si10.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_si10 = pd.read_parquet(\"./notebooks/weather/all_si10_regions.parquet\")\n",
    "all_ssrd = pd.read_parquet(\"./notebooks/weather/all_ssrd_regions.parquet\")\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "all_si10.plot(ax=ax1)\n",
    "all_ssrd.plot(ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing all available forcast horizons\n",
    "\n",
    "In order to asses the performance of the forecast prediction, we will extract all the available forcasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "import shapely.plotting\n",
    "import geojson\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pickle\n",
    "\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metropole_geojson_file = \"./notebooks/datascience/metropole.geojson\"\n",
    "metropole = geojson.load(open(metropole_geojson_file))\n",
    "polys_france = [Polygon(geo[0]) for geo in metropole[\"geometry\"][\"coordinates\"]]\n",
    "areas = [p.area for p in polys_france]\n",
    "\n",
    "min_area = 0.2  # threshold to remove small islands\n",
    "polys_france = [p for p, area in zip(polys_france, areas) if area > min_area]\n",
    "print(f\"number of polygons: {len(polys_france)}\")\n",
    "poly_france = MultiPolygon(polys_france)\n",
    "# poly_france = polys_france[0]  # france metropolitan\n",
    "\n",
    "bouns = poly_france.envelope.bounds\n",
    "min_lon = bouns[0]\n",
    "max_lon = bouns[2]\n",
    "min_lat = bouns[1]\n",
    "max_lat = bouns[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_masks = pickle.load(open(\"mask.pkl\", \"rb\"))\n",
    "mask_france_wind = the_masks[\"mask\"]\n",
    "masks_regions = the_masks[\"masks_regions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# apply formatter to the logger\n",
    "logger.handlers[0].setFormatter(formatter)\n",
    "\n",
    "\n",
    "logging.info(\"This is an info message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2022-02-01\"\n",
    "end_date = \"2024-04-08\"\n",
    "list_date = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "list_date = list_date.strftime(\"%Y-%m-%d\")\n",
    "cases_forecast = {\n",
    "    \"J+0\": [\"00H12H\", \"13H24H\"],\n",
    "    \"J+1\": [\"25H36H\", \"37H48H\"],\n",
    "    \"J+2\": [\"49H60H\", \"61H72H\"],\n",
    "    \"J+3\": [\"73H84H\", \"85H96H\"],\n",
    "}\n",
    "cases = [\n",
    "    \"00H12H\",\n",
    "    \"13H24H\",\n",
    "]  # \"25H36H\", \"37H48H\", \"49H60H\", \"61H72H\", \"73H84H\", \"85H96H\", \"97H102H\"]\n",
    "filename_to_save_template = (\n",
    "    \"/shared/home/antoine-2etavant/data/arpege/{date}_SP1_{case}.grib2\"\n",
    ")\n",
    "\n",
    "KEYS_FILTER_SSPD = {\n",
    "    \"typeOfLevel\": \"surface\",\n",
    "    \"cfVarName\": \"ssrd\",\n",
    "}\n",
    "KEYS_FILTER_WIND = {\n",
    "    \"typeOfLevel\": \"heightAboveGround\",\n",
    "    \"level\": 10,\n",
    "    \"cfVarName\": \"si10\",\n",
    "}\n",
    "KEYS_FILTER_T2M = {\n",
    "    \"typeOfLevel\": \"heightAboveGround\",\n",
    "    \"level\": 2,\n",
    "    \"cfVarName\": \"t2m\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_filenames_from_case(horizon: str):\n",
    "    morning_case, afternoon_case = cases_forecast[horizon]\n",
    "    filenames_afternoon = [\n",
    "        filename_to_save_template.format(date=date, case=afternoon_case)\n",
    "        for date in list_date\n",
    "    ]\n",
    "    filenames_afternoon_exist = [\n",
    "        filename for filename in filenames_afternoon if Path(filename).exists()\n",
    "    ]\n",
    "    filenames_afternoon = filenames_afternoon_exist\n",
    "    filenames_morning = [\n",
    "        filename_to_save_template.format(date=date, case=morning_case)\n",
    "        for date in list_date\n",
    "    ]\n",
    "    filenames_morning_exist = [\n",
    "        filename for filename in filenames_morning if Path(filename).exists()\n",
    "    ]\n",
    "    filenames_morning = filenames_morning_exist\n",
    "    return filenames_morning, filenames_afternoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_filenames(filenames, KEYS_FILTER):\n",
    "    return [\n",
    "        filename\n",
    "        for filename in filenames\n",
    "        if xr.open_dataset(\n",
    "            filename, engine=\"cfgrib\", backend_kwargs={\"filter_by_keys\": KEYS_FILTER}\n",
    "        ).data_vars\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_no_step(ds):\n",
    "    # Check if 'step' is in the dataset's coordinates\n",
    "    # Used to find files that are not working properly\n",
    "    if \"step\" not in ds.coords:\n",
    "        display(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ssrd(\n",
    "    horizon: str,\n",
    "    mask_france=mask_france_wind,\n",
    "    masks_regions=masks_regions,\n",
    "    bounds_france=[min_lon, max_lon, min_lat, max_lat],\n",
    "    save_parquet=False,\n",
    "    process_regions=False,\n",
    "):\n",
    "    _, filenames_afternoon = list_of_filenames_from_case(horizon)\n",
    "    filenames_afternoon = filter_filenames(filenames_afternoon, KEYS_FILTER_SSPD)\n",
    "\n",
    "    logging.info(f\"Processing {horizon} with {len(filenames_afternoon)} files\")\n",
    "    da_ssrd: xr.DataArray = xr.open_mfdataset(\n",
    "        filenames_afternoon,\n",
    "        engine=\"cfgrib\",\n",
    "        parallel=True,\n",
    "        backend_kwargs={\"filter_by_keys\": KEYS_FILTER_SSPD},\n",
    "        concat_dim=\"time\",\n",
    "        combine=\"nested\",\n",
    "        preprocess=drop_no_step,\n",
    "    ).ssrd\n",
    "    min_lon, max_lon, min_lat, max_lat = bounds_france\n",
    "    if horizon == \"J+0\":\n",
    "        step = np.timedelta64(1, \"D\")\n",
    "    elif horizon == \"J+1\":\n",
    "        step = np.timedelta64(2, \"D\")\n",
    "    elif horizon == \"J+2\":\n",
    "        step = np.timedelta64(3, \"D\")\n",
    "    elif horizon == \"J+3\":\n",
    "        step = np.timedelta64(4, \"D\")\n",
    "    ssrd_france_large = da_ssrd.sel(step=step).where(\n",
    "        (da_ssrd.longitude > min_lon)\n",
    "        & (da_ssrd.longitude < max_lon)\n",
    "        & (da_ssrd.latitude > min_lat)\n",
    "        & (da_ssrd.latitude < max_lat),\n",
    "        drop=True,\n",
    "    )\n",
    "    total_sun_flux_large = ssrd_france_large.sum(dim=[\"latitude\", \"longitude\"])\n",
    "    logging.info(f\"Total sun flux large: {total_sun_flux_large}\")\n",
    "    ssrd_france_exact = ssrd_france_large.where(mask_france, drop=True)\n",
    "    total_sun_flux_exact = ssrd_france_exact.sum(dim=[\"latitude\", \"longitude\"])\n",
    "    logging.info(f\"Total sun flux exact: {total_sun_flux_exact}\")\n",
    "\n",
    "    if process_regions:\n",
    "        results_ssrd = {}\n",
    "        for name, mask in tqdm(masks_regions.items(), leave=False):\n",
    "            ssrd = ssrd_france_large.where(mask, drop=True)\n",
    "            ssrd_daily = ssrd.sum(dim=[\"latitude\", \"longitude\"]).to_dataframe()[\"ssrd\"]\n",
    "            results_ssrd[name] = ssrd_daily\n",
    "\n",
    "        all_ssrd = pd.concat(results_ssrd, axis=1)\n",
    "\n",
    "    if save_parquet:\n",
    "        prefix = (\n",
    "            \"./notebooks/weather/forcasts/\"\n",
    "            + horizon\n",
    "            + \"_\"\n",
    "            + str(start_date)\n",
    "            + \"_\"\n",
    "            + str(end_date)\n",
    "            + \"_\"\n",
    "        )\n",
    "        if process_regions:\n",
    "            logging.info(f\"Saving to {prefix}\")\n",
    "            all_ssrd.to_parquet(prefix + \"sun_flux_regions.parquet\")\n",
    "            logging.info(\"Saved to regions\")\n",
    "        else:\n",
    "            # total_sun_flux_large.to_dataframe()[[\"ssrd\"]].to_parquet(\n",
    "            #     prefix + \"sun_flux_large.parquet\"\n",
    "            # )\n",
    "            # logging.info(\"Saved to large\")\n",
    "            total_sun_flux_exact.to_dataframe()[[\"ssrd\"]].to_parquet(\n",
    "                prefix + \"sun_flux_exact.parquet\"\n",
    "            )\n",
    "            logging.info(\"Saved to exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wind(\n",
    "    horizon: str,\n",
    "    mask_france=mask_france_wind,\n",
    "    masks_regions=masks_regions,\n",
    "    bounds_france=[min_lon, max_lon, min_lat, max_lat],\n",
    "    save_parquet=False,\n",
    "    process_regions=False,\n",
    "):\n",
    "    filenames_morning, filenames_afternoon = list_of_filenames_from_case(horizon)\n",
    "    filenames_morning = filter_filenames(filenames_morning, KEYS_FILTER_WIND)\n",
    "    filenames_afternoon = filter_filenames(filenames_afternoon, KEYS_FILTER_WIND)\n",
    "\n",
    "    logging.info(f\"Processing {horizon} with {len(filenames_morning)} files\")\n",
    "    da_wind_10m_morning = xr.open_mfdataset(\n",
    "        filenames_morning,\n",
    "        engine=\"cfgrib\",\n",
    "        parallel=True,\n",
    "        backend_kwargs={\"filter_by_keys\": KEYS_FILTER_WIND},\n",
    "        concat_dim=\"time\",\n",
    "        combine=\"nested\",\n",
    "    ).si10\n",
    "    da_wind_10m_afternoon = xr.open_mfdataset(\n",
    "        filenames_afternoon,\n",
    "        engine=\"cfgrib\",\n",
    "        parallel=True,\n",
    "        backend_kwargs={\"filter_by_keys\": KEYS_FILTER_WIND},\n",
    "        concat_dim=\"time\",\n",
    "        combine=\"nested\",\n",
    "    ).si10\n",
    "    da_wind_10m = xr.concat(\n",
    "        [da_wind_10m_morning, da_wind_10m_afternoon],\n",
    "        dim=\"step\",\n",
    "    )\n",
    "    min_lon, max_lon, min_lat, max_lat = bounds_france\n",
    "    wind_10m_large = da_wind_10m.where(\n",
    "        (da_wind_10m.longitude > min_lon)\n",
    "        & (da_wind_10m.longitude < max_lon)\n",
    "        & (da_wind_10m.latitude > min_lat)\n",
    "        & (da_wind_10m.latitude < max_lat),\n",
    "        drop=True,\n",
    "    )\n",
    "    total_wind_10m_large = (\n",
    "        wind_10m_large.sum(dim=[\"latitude\", \"longitude\"])\n",
    "        .stack(time_step=(\"time\", \"step\"))\n",
    "        .set_index(time_step=\"valid_time\")\n",
    "    )\n",
    "    logging.info(f\"Total wind 10m large: {total_wind_10m_large}\")\n",
    "\n",
    "    wind_10m_exact = wind_10m_large.where(mask_france, drop=True)\n",
    "    total_wind_10m_exact = (\n",
    "        wind_10m_exact.sum(dim=[\"latitude\", \"longitude\"])\n",
    "        .stack(time_step=(\"time\", \"step\"))\n",
    "        .set_index(time_step=\"valid_time\")\n",
    "    )\n",
    "    logging.info(f\"Total wind 10m exact: {total_wind_10m_exact}\")\n",
    "\n",
    "    if process_regions:\n",
    "        results_si10 = {}\n",
    "        for name, mask in tqdm(masks_regions.items(), leave=False):\n",
    "            wind = wind_10m_large.where(mask, drop=True)\n",
    "            wind_daily = (\n",
    "                wind.sum(dim=[\"latitude\", \"longitude\"])\n",
    "                .stack(time_step=(\"time\", \"step\"))\n",
    "                .set_index(time_step=\"valid_time\")\n",
    "                .to_dataframe()[\"si10\"]\n",
    "            )\n",
    "            results_si10[name] = wind_daily\n",
    "\n",
    "        all_si10 = pd.concat(results_si10, axis=1)\n",
    "\n",
    "    if save_parquet:\n",
    "        prefix = (\n",
    "            \"./notebooks/weather/forcasts/\"\n",
    "            + horizon\n",
    "            + \"_\"\n",
    "            + str(start_date)\n",
    "            + \"_\"\n",
    "            + str(end_date)\n",
    "            + \"_\"\n",
    "        )\n",
    "        logging.info(f\"Saving to {prefix}\")\n",
    "        if process_regions:\n",
    "            all_si10.to_parquet(prefix + \"wind_regions.parquet\")\n",
    "            logging.info(\"Saved to regions\")\n",
    "        else:\n",
    "            # total_wind_10m_large.to_dataframe()[[\"si10\"]].to_parquet(\n",
    "            #     prefix + \"wind_large.parquet\"\n",
    "            # )\n",
    "            # logging.info(\"Saved to large\")\n",
    "            total_wind_10m_exact.to_dataframe()[[\"si10\"]].to_parquet(\n",
    "                prefix + \"wind_exact.parquet\"\n",
    "            )\n",
    "            logging.info(\"Saved to exact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pbar.unregister()\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in [\"J+1\", \"J+2\", \"J+3\"]:\n",
    "    start_date = \"2022-02-01\"\n",
    "    end_date = \"2023-01-15\"\n",
    "    list_date = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "    list_date = list_date.strftime(\"%Y-%m-%d\")\n",
    "    logging.info(f\"Processing {horizon} {start_date} to {end_date}\")\n",
    "    process_ssrd(horizon, save_parquet=True)\n",
    "    process_wind(horizon, save_parquet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in [\"J+0\", \"J+1\", \"J+2\", \"J+3\"]:\n",
    "    start_date = \"2023-01-15\"\n",
    "    end_date = \"2024-04-08\"\n",
    "    list_date = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "    list_date = list_date.strftime(\"%Y-%m-%d\")\n",
    "    logging.info(f\"Processing {horizon} {start_date} to {end_date}\")\n",
    "    process_ssrd(horizon, save_parquet=True)\n",
    "    process_wind(horizon, save_parquet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat the forecasts to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {}\n",
    "for horizon in [\"J+0\", \"J+1\", \"J+2\", \"J+3\"]:\n",
    "    start_date = \"2022-02-01\"\n",
    "    end_date = \"2023-01-15\"\n",
    "    prefix = (\n",
    "        \"./notebooks/weather/forcasts/\"\n",
    "        + horizon\n",
    "        + \"_\"\n",
    "        + str(start_date)\n",
    "        + \"_\"\n",
    "        + str(end_date)\n",
    "        + \"_\"\n",
    "    )\n",
    "    sun_exact_filename = prefix + \"sun_flux_exact.parquet\"\n",
    "    wind_exact_filename = prefix + \"wind_exact.parquet\"\n",
    "    datas[horizon + \"_sun_exact\"] = pd.read_parquet(sun_exact_filename)\n",
    "    datas[horizon + \"_wind_exact\"] = pd.read_parquet(wind_exact_filename)\n",
    "\n",
    "    start_date = \"2023-01-15\"\n",
    "    end_date = \"2024-04-08\"\n",
    "    prefix = (\n",
    "        \"./notebooks/weather/forcasts/\"\n",
    "        + horizon\n",
    "        + \"_\"\n",
    "        + str(start_date)\n",
    "        + \"_\"\n",
    "        + str(end_date)\n",
    "        + \"_\"\n",
    "    )\n",
    "    sun_exact_filename = prefix + \"sun_flux_exact.parquet\"\n",
    "    wind_exact_filename = prefix + \"wind_exact.parquet\"\n",
    "    datas[horizon + \"_sun_exact\"] = pd.concat(\n",
    "        [\n",
    "            datas[horizon + \"_sun_exact\"],\n",
    "            pd.read_parquet(sun_exact_filename).loc[\"2023-01-16\":],\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    try:\n",
    "        df2_wind = pd.read_parquet(wind_exact_filename).loc[\"2023-01-16\":]\n",
    "    except KeyError:\n",
    "        df2_wind = pd.read_parquet(wind_exact_filename)\n",
    "    datas[horizon + \"_wind_exact\"] = pd.concat(\n",
    "        [datas[horizon + \"_wind_exact\"], df2_wind[~df2_wind.index.duplicated()]], axis=0\n",
    "    )\n",
    "\n",
    "    datas[horizon + \"_sun_exact\"].rename(\n",
    "        columns={\"ssrd\": horizon + \"_sun_exact\"}, inplace=True\n",
    "    )\n",
    "    datas[horizon + \"_wind_exact\"].rename(\n",
    "        columns={\"si10\": horizon + \"_wind_exact\"}, inplace=True\n",
    "    )\n",
    "    datas[horizon + \"_wind_exact\"] = datas[horizon + \"_wind_exact\"].resample(\"D\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_forecasts = pd.concat(datas.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sun flux is accumulated from the very beginning of the forecast, not the day !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_forecasts[\"J+3_sun_exact\"] -= mean_forecasts[\"J+2_sun_exact\"]\n",
    "mean_forecasts[\"J+2_sun_exact\"] -= mean_forecasts[\"J+1_sun_exact\"]\n",
    "mean_forecasts[\"J+1_sun_exact\"] -= mean_forecasts[\"J+0_sun_exact\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_forecasts.to_csv(\"./notebooks/weather/mean_forecasts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_forecasts.filter(like=\"sun\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_forecasts.filter(like=\"wind\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
